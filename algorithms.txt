https://www.geeksforgeeks.org/fundamentals-of-algorithms/

1) O(1): Time complexity of a function (or set of statements) is considered as O(1) if it doesn’t contain loop, recursion and call to any other non-constant time function.

   // set of non-recursive and non-loop statements

O(Logn) Time Complexity of a loop is considered as O(Logn) if the loop variables is divided / multiplied by a constant amount.
for (int i = 1; i <=n; i *= c) {
       // some O(1) expressions
   }
   for (int i = n; i > 0; i /= c) {
       // some O(1) expressions
   }

Solving Recurrences
===================
 There are mainly three ways for solving recurrences.

1) Substitution Method: We make a guess for the solution and then we use mathematical induction to prove the guess is correct or incorrect.

For example consider the recurrence T(n) = 2T(n/2) + n

We guess the solution as T(n) = O(nLogn). Now we use induction
to prove our guess.

We need to prove that T(n) <= cnLogn. We can assume that it is true
for values smaller than n.

T(n) = 2T(n/2) + n
    <= cn/2Log(n/2) + n
    =  cnLogn - cnLog2 + n
    =  cnLogn - cn + n
    <= cnLogn

2) Recurrence Tree Method: In this method, we draw a recurrence tree and calculate the time taken by every level of tree. Finally, we sum the work done at all levels. To draw the recurrence tree, we start from the given recurrence and keep drawing till we find a pattern among levels. The pattern is typically a arithmetic or geometric series.

For example consider the recurrence relation 
T(n) = T(n/4) + T(n/2) + cn2

           cn2
         /      \
     T(n/4)     T(n/2)

If we further break down the expression T(n/4) and T(n/2), 
we get following recursion tree.

                cn2
           /           \      
       c(n2)/16      c(n2)/4
      /      \          /     \
  T(n/16)     T(n/8)  T(n/8)    T(n/4) 
Breaking down further gives us following
                 cn2
            /            \      
       c(n2)/16          c(n2)/4
       /      \            /      \
c(n2)/256   c(n2)/64  c(n2)/64    c(n2)/16
 /    \      /    \    /    \       /    \  

To know the value of T(n), we need to calculate sum of tree 
nodes level by level. If we sum the above tree level by level, 
we get the following series
T(n)  = c(n^2 + 5(n^2)/16 + 25(n^2)/256) + ....
The above series is geometrical progression with ratio 5/16.

To get an upper bound, we can sum the infinite series. 
We get the sum as (n2)/(1 - 5/16) which is O(n2)

3) Master Method:
Master Method is a direct way to get the solution. The master method works only for following type of recurrences or for recurrences that can be transformed to following type.

T(n) = aT(n/b) + f(n) where a >= 1 and b > 1

There are following three cases:
1. If f(n) = Θ(nc) where c < Logba then T(n) = Θ(nLogba)

2. If f(n) = Θ(nc) where c = Logba then T(n) = Θ(ncLog n)

3.If f(n) = Θ(nc) where c > Logba then T(n) = Θ(f(n))

Amortized Analysis
==================
Amortized Analysis is used for algorithms where an occasional operation is very slow, but most of the other operations are faster. In Amortized Analysis, we analyze a sequence of operations and guarantee a worst case average time which is lower than the worst case time of a particular expensive operation.

example
--------
Dynamic Table of HashMap/HashTable. The idea is to increase size of table whenever it becomes full. Following are the steps to follow when table becomes full.
1) Allocate memory for a larger table of size, typically twice the old table.
2) Copy the contents of old table to new table.
3) Free the old table.

If the table has space available, we simply insert new item in available space.

What is the time complexity of n insertions using the above scheme?
If we use simple analysis, the worst case cost of an insertion is O(n). Therefore, worst case cost of n inserts is n * O(n) which is O(n2). This analysis gives an upper bound, but not a tight upper bound for n insertions as all insertions don’t take Θ(n) time.

So using Amortized Analysis, we could prove that the Dynamic Table scheme has O(1) insertion time which is a great result used in hashing. Also, the concept of dynamic table is used in vectors in C++, ArrayList in Java.

Space Complexity
================
Following are the correct definitions of Auxiliary Space and Space Complexity.
Auxiliary Space is the extra space or temporary space used by an algorithm.

Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input.

For example, if we want to compare standard sorting algorithms on the basis of space, then Auxiliary Space would be a better criteria than Space Complexity. Merge Sort uses O(n) auxiliary space, Insertion sort and Heap Sort use O(1) auxiliary space. Space complexity of all these sorting algorithms is O(n) though.

A Time Complexity Question
----------------
void fun() 
{ 
   int i, j; 
   for (i=1; i<=n; i++) 
      for (j=1; j<=log(i); j++) 
         printf("GeeksforGeeks"); 
}
Time Complexity of the above function can be written as Θ(log 1) + Θ(log 2) + Θ(log 3) + . . . . + Θ(log n) which is Θ (log n!). i.e., Θ (log n!) = Θ(n log n). So time complexity of fun() is Θ(n log n).


void fun(int n) 
{ 
   int j = 1, i = 0; 
   while (i < n) 
   { 
       // Some O(1) task 
       i = i + j; 
       j++; 
   } 
} 
The value of i is x(x+1)/2 after x iterations. So if loop runs x times, then x(x+1)/2 < n. Therefore time complexity can be written as Θ(√n). 

void fun(int n, int k) 
{ 
    for (int i=1; i<=n; i++) 
    { 
      int p = pow(i, k);  
      for (int j=1; j<=p; j++) 
      { 
          // Some O(1) work 
      } 
    } 
} 
 time complexity is Θ(nk+1 / (k+1)).


// Function 1 
int fun1(int arr[R][C]) 
{ 
    int sum = 0; 
    for (int i=0; i<R; i++) 
      for (int j=0; j<C; j++) 
          sum += arr[i][j]; 
} 
  
// Function 2 
int fun2(int arr[R][C]) 
{ 
    int sum = 0; 
    for (int j=0; j<C; j++) 
      for (int i=0; i<R; i++) 
          sum += arr[i][j]; 
} 
In C/C++, elements are stored in Row-Major order. So the first implementation has better spatial locality (nearby memory locations are referenced in successive iterations). Therefore, first implementation should always be preferred for iterating multidimensional arrays.

Jump Search
===========
Like Binary Search, Jump Search is a searching algorithm for sorted arrays. The basic idea is to check fewer elements (than linear search) by jumping ahead by fixed steps or skipping some elements in place of searching all elements.

Let’s consider the following array: (0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610). Length of the array is 16. Jump search will find the value of 55 with the following steps assuming that the block size to be jumped is 4.
STEP 1: Jump from index 0 to index 4;
STEP 2: Jump from index 4 to index 8;
STEP 3: Jump from index 8 to index 12;
STEP 4: Since the element at index 12 is greater than 55 we will jump back a step to come to index 8.
STEP 5: Perform linear search from index 8 to get the element 55.

What is the optimal block size to be skipped?
In the worst case, we have to do n/m jumps and if the last checked value is greater than the element to be searched for, we perform m-1 comparisons more for linear search. Therefore the total number of comparisons in the worst case will be ((n/m) + m-1). The value of the function ((n/m) + m-1) will be minimum when m = √n. Therefore, the best step size is m = √n.


Interpolation Search
====================
Given a sorted array of n uniformly distributed values arr[], write a function to search for a particular element x in the array.
// The idea of formula is to return higher value of pos
// when element to be searched is closer to arr[hi]. And
// smaller value when closer to arr[lo]
pos = lo + [ (x-arr[lo])*(hi-lo) / (arr[hi]-arr[Lo]) ]

arr[] ==> Array where elements need to be searched
x     ==> Element to be searched
lo    ==> Starting index in arr[]
hi    ==> Ending index in arr[]

Algorithm
Step1: In a loop, calculate the value of “pos” using the probe position formula.
Step2: If it is a match, return the index of the item, and exit.
Step3: If the item is less than arr[pos], calculate the probe position of the left sub-array. Otherwise calculate the same in the right sub-array.
Step4: Repeat until a match is found or the sub-array reduces to zero. 

Time Complexity: If elements are uniformly distributed, then O (log log n)). In worst case it can take upto O(n).
Auxiliary Space: O(1)

Exponential search 
==================
involves two steps:
    Find range where element is present
    Do Binary Search in above found range.

How to find the range where element may be present?
The idea is to start with subarray size 1, compare its last element with x, then try size 2, then 4 and so on until last element of a subarray is not greater.
Once we find an index i (after repeated doubling of i), we know that the element must be present between i/2 and i .

Time Complexity : O(Log n)
Auxiliary Space : The above implementation of Binary Search is recursive and requires O(Log n) space. With iterative Binary Search, we need only O(1) space.

Applications of Exponential Search:

    Exponential Binary Search is particularly useful for unbounded searches, where size of array is infinite. Please refer Unbounded Binary Search for an example.
    It works better than Binary Search for bounded arrays, and also when the element to be searched is closer to the first element.
Ternary Search
===============
 int mid1 = l + (r - l)/3; 
 int mid2 = mid1 + (r - l)/3; 

 Which of the above two does less comparisons in worst case?
 e Complexity for Binary search = 2clog2n + O(1)
Time Complexity for Ternary search = 4clog3n + O(1)
Therefore, the comparison of Ternary and Binary Searches boils down the comparison of expressions 2Log3n and Log2n . The value of 2Log3n can be written as (2 / Log23) * Log2n . Since the value of (2 / Log23) is more than one, Ternary Search does more comparisons than Binary Search in worst case.

Exercise:
Why Merge Sort divides input array in two halves, why not in three or more parts?

Selection Sort
===============
The selection sort algorithm sorts an array by repeatedly finding the minimum element (considering ascending order) from unsorted part and putting it at the beginning.
arr[] = 64 25 12 22 11
// Find the minimum element in arr[0...4]
// and place it at beginning
11 25 12 22 64
// Find the minimum element in arr[1...4]
// and place it at beginning of arr[1...4]
11 12 25 22 64
// Find the minimum element in arr[2...4]
// and place it at beginning of arr[2...4]
11 12 22 25 64
// Find the minimum element in arr[3...4]
// and place it at beginning of arr[3...4]
11 12 22 25 64 
Time Complexity: O(n^2) as there are two nested loops.
Auxiliary Space: O(1)
The good thing about selection sort is it never makes more than O(n) swaps and can be useful when memory write is a costly operation


Bubble Sort
===========
Bubble Sort is the simplest sorting algorithm that works by repeatedly swapping the adjacent elements if they are in wrong order.
Example:
First Pass:
( 5 1 4 2 8 ) –> ( 1 5 4 2 8 ), Here, algorithm compares the first two elements, and swaps since 5 > 1.
( 1 5 4 2 8 ) –>  ( 1 4 5 2 8 ), Swap since 5 > 4
( 1 4 5 2 8 ) –>  ( 1 4 2 5 8 ), Swap since 5 > 2
( 1 4 2 5 8 ) –> ( 1 4 2 5 8 ), Now, since these elements are already in order (8 > 5), algorithm does not swap them.
Second Pass:
( 1 4 2 5 8 ) –> ( 1 4 2 5 8 )
( 1 4 2 5 8 ) –> ( 1 2 4 5 8 ), Swap since 4 > 2
( 1 2 4 5 8 ) –> ( 1 2 4 5 8 )
( 1 2 4 5 8 ) –>  ( 1 2 4 5 8 )
Now, the array is already sorted, but our algorithm does not know if it is completed. The algorithm needs one whole pass without any swap to know it is sorted.
Third Pass:
( 1 2 4 5 8 ) –> ( 1 2 4 5 8 )
( 1 2 4 5 8 ) –> ( 1 2 4 5 8 )
( 1 2 4 5 8 ) –> ( 1 2 4 5 8 )
( 1 2 4 5 8 ) –> ( 1 2 4 5 8 )
Optimized Implementation:
The above function always runs O(n^2) time even if the array is sorted. It can be optimized by stopping the algorithm if inner loop didn’t cause any swap.

Worst and Average Case Time Complexity: O(n*n). Worst case occurs when array is reverse sorted.
Best Case Time Complexity: O(n). Best case occurs when array is already sorted.
Auxiliary Space: O(1).


Insertion Sort
===============
Insertion sort is a simple sorting algorithm that works the way we sort playing cards in our hands.
12, 11, 13, 5, 6

Let us loop for i = 1 (second element of the array) to 4 (last element of the array)

i = 1. Since 11 is smaller than 12, move 12 and insert 11 before 12
11, 12, 13, 5, 6

i = 2. 13 will remain at its position as all elements in A[0..I-1] are smaller than 13
11, 12, 13, 5, 6

i = 3. 5 will move to the beginning and all other elements from 11 to 13 will move one position ahead of their current position.
5, 11, 12, 13, 6

i = 4. 6 will move to position after 5, and elements from 11 to 13 will move one position ahead of their current position.
5, 6, 11, 12, 13

Time Complexity: O(n*2)
Auxiliary Space: O(1)
Boundary Cases: Insertion sort takes maximum time to sort if elements are sorted in reverse order. And it takes minimum time (Order of n) when elements are already sorted.
Merge Sort
==========
Time complexity of Merge Sort is \Theta(nLogn) in all 3 cases (worst, average and best) 
Auxiliary Space: O(n)
O(1) auxiliary space with linked lists[
HeapSort
========
Time Complexity: Time complexity of heapify is O(Logn). Time complexity of createAndBuildHeap() is O(n) and overall time complexity of Heap Sort is O(nLogn).

Applications of HeapSort
1. Sort a nearly sorted (or K sorted) array
2. k largest(or smallest) elements in an array 

Auxiliary Space O(1)

Quick Sort
==========
time:
-----
Worst-case performance	O(n2)
Best-case performance	O(n log n) (simple partition)
or O(n) (three-way partition and equal keys)
Average performance	O(n log n)
space
-----
Worst-case space complexity	O(n) auxiliary (naive)
O(log n) auxiliary (Sedgewick 1978)

TODO : analysis all three cases and code
analysis on randomization quicksort

Counting Sort
=============
Counting sort is a sorting technique based on keys between a specific range.
For simplicity, consider the data in the range 0 to 9. 
Input data: 1, 4, 1, 2, 7, 5, 2
  1) Take a count array to store the count of each unique object.
  Index:     0  1  2  3  4  5  6  7  8  9
  Count:     0  2  2  0   1  1  0  1  0  0

  2) Modify the count array such that each element at each index 
  stores the sum of previous counts. 
  Index:     0  1  2  3  4  5  6  7  8  9
  Count:     0  2  4  4  5  6  6  7  7  7

The modified count array indicates the position of each object in 
the output sequence.
 
  3) Output each object from the input sequence followed by 
  decreasing its count by 1.
  Process the input data: 1, 4, 1, 2, 7, 5, 2. Position of 1 is 2.
  Put data 1 at index 2 in output. Decrease count by 1 to place 
  next data 1 at an index 1 smaller than this index.

Time Complexity: O(n+k) where n is the number of elements in input array and k is the range of input.
Auxiliary Space: O(n+k)

The problem with the previous counting sort was that we could not sort the elements if we have negative numbers in it. Because there are no negative array indices. So what we do is, we find the minimum element and we will store count of that minimum element at zero index.

Points to be noted:
1. Counting sort is efficient if the range of input data is not significantly greater than the number of objects to be sorted. Consider the situation where the input sequence is between range 1 to 10K and the data is 10, 5, 10K, 5K.
2. It is not a comparison based sorting. It running time complexity is O(n) with space proportional to the range of data.
3. It is often used as a sub-routine to another sorting algorithm like radix sort.
4. Counting sort uses a partial hashing to count the occurrence of the data object in O(1).
5. Counting sort can be extended to work for negative inputs also.

Exercise:
1. Modify above code to sort the input data in the range from M to N.
2. Is counting sort stable and online?
3. Thoughts on parallelizing the counting sort algorithm.

 

Radix sort
==========
The idea of Radix Sort is to do digit by digit sort starting from least significant digit to most significant digit. Radix sort uses counting sort as a subroutine to sort.
Worst-case performance	O ( w ⋅ n ) where w is the number of bits required to store each key.
Worst-case space complexity	O ( w + n ) ( where w space ,beacuse group by bit places and n for space need for counting sort )
Example:
Original, unsorted list:
 170, 45, 75, 90, 802, 24, 2, 66 
 Sorting by least significant digit (1s place) gives
 170, 90, 802, 2, 24, 45, 75, 66
 Sorting by next digit (10s place) gives
 802, 2, 24, 45, 66, 170, 75, 90
 Sorting by most significant digit (100s place) gives:
 2, 24, 45, 66, 75, 90, 170, 802 


 Timsort
 ========
 Worst-case performance	O ( n log ⁡ n ) 
Best-case performance	O ( n ) 
Average performance	O ( n log ⁡ n )
Worst-case space complexity	O ( n ) 

TimSort is a sorting algorithm based on Insertion Sort and Merge Sort.
    A stable sorting algorithm works in O(n Log n) time
    Used in Java’s Arrays.sort() as well as Python’s sorted() and sort().
    First sort small pieces using Insertion Sort, then merges the pieces using merge of merge sort.
We divide the Array into blocks known as Run. We sort those runs using insertion sort one by one and then merge those runs using combine function used in merge sort. If the size of Array is less than run, then Array get sorted just by using Insertion Sort. The size of run may vary from 32 to 64 depending upon the size of the array. Note that merge function performs well when sizes subarrays are powers of 2. The idea is based on the fact that insertion sort performs well for small arrays.

Bucket Sort
============
Bucket sort is mainly useful when input is uniformly distributed over a range. 
Sort a large set of floating point numbers which are in range from 0.0 to 1.0 and are uniformly distributed across the range. How do we sort the numbers efficiently?

Can we sort the array in linear time? Counting sort can not be applied here as we use keys as index in counting sort. Here keys are floating point numbers. 

bucketSort(arr[], n)
1) Create n empty buckets (Or lists).
2) Do following for every array element arr[i].
.......a) Insert arr[i] into bucket[n*array[i]]
3) Sort individual buckets using insertion sort.
4) Concatenate all sorted buckets.

insertion is simillary to java hashMap. if collision happen, then they will store linked list with sorting order.

example https://www.geeksforgeeks.org/bucket-sort-2/

Worst-case performance	O ( n^2 ) 
Average performance	O ( n + n^2/k + k ) , where k is the number of buckets. 
O ( n ) , when  k ≈ n 
Worst-case space complexity	O ( n ⋅ k ) 

Pigeonhole sort
===============
Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements and the number of possible keys are approximately the same.
It requires O(n + Range) time where n is number of elements in input array and ‘Range’ is number of possible values in array. 
Worst-case space complexity	O ( n+R ) 
Working of Algorithm :
    Find minimum and maximum values in array. Let the minimum and maximum values be ‘min’ and ‘max’ respectively. Also find range as ‘max-min-1’.
    Set up an array of initially empty “pigeonholes” the same size as of the range.
    Visit each element of the array and then put each element in its pigeonhole. An element arr[i] is put in hole at index arr[i] – min.
    Start the loop all over the pigeonhole array in order and put the elements from non- empty holes back into the original array.
Comparison with Counting Sort :
It is similar to counting sort, but differs in that it “moves items twice: once to the bucket array and again to the final destination “. 


Cycle Sort
===========
Cycle sort is an in-place sorting Algorithm, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array. 
Explanation :
arr[] = {10, 5, 2, 3}
 index =  0   1   2   3
 for (int cycle_start = 0; cycle_start <= n - 2; cycle_start++) { 
            // initialize item as starting point 
            int item = arr[cycle_start]; 
  
            // Find position where we put the item. We basically 
            // count all smaller elements on right side of item. 
            int pos = cycle_start; 
            for (int i = cycle_start + 1; i < n; i++) 
                if (arr[i] < item) 
                    pos++; 

We put 10 at arr[3] and change item to 
old value of arr[3].
arr[] = {10, 5, 2, 10} 
item = 3 

Again rotate rest cycle that start with index '0' 
Find position where we put the item = 3 
we swap item with element at arr[1] now 
arr[] = {10, 3, 2, 10} 
item = 5

Again rotate rest cycle that start with index '0' and item = 5 
we swap item with element at arr[2].
arr[] = {10, 3, 5, 10 } 
item = 2

Again rotate rest cycle that start with index '0' and item = 2
arr[] = {2, 3,  5, 10}  

Above is one iteration for cycle_stat = 0.
Repeat above steps for cycle_start = 1, 2, ..n-2


Binary Insertion Sort
======================
We can use binary search to reduce the number of comparisons in normal insertion sort. Binary Insertion Sort uses binary search to find the proper location to insert the selected item at each iteration.
In normal insertion sort, it takes O(n) comparisons(at nth iteration) in worst case. We can reduce it to O(log n) by using binary search.

Time Complexity: The algorithm as a whole still has a running worst case running time of O(n2) because of the series of swaps required for each insertion. 

Why Quick Sort preferred for Arrays and Merge Sort for Linked Lists?
==============================================
Why is Quick Sort preferred for arrays?
Quick Sort in its general form is an in-place sort (i.e. it doesn’t require any extra storage) whereas merge sort requires O(N) extra storage, N denoting the array size which may be quite expensive.
Most practical implementations of Quick Sort use randomized version. The randomized version has expected time complexity of O(nLogn). The worst case is possible in randomized version also, but worst case doesn’t occur for a particular pattern (like sorted array) and randomized Quick Sort works well in practice. 
Quick Sort is also a cache friendly sorting algorithm as it has good locality of reference when used for arrays.
Quick Sort is also tail recursive, therefore tail call optimizations is done.

Why is Merge Sort preferred for Linked Lists?
TODO

Stability in sorting algorithms
===============================
A sorting algorithm is said to be stable if two objects with equal keys appear in the same order in sorted output as they appear in the input array to be sorted.


Find the Minimum length Unsorted Subarray, sorting which makes the complete array sorted
===============================
Given an unsorted array arr[0..n-1] of size n, find the minimum length subarray arr[s..e] such that sorting this subarray makes the whole array sorted.
Examples:
1) If the input array is [10, 12, 20, 30, 25, 40, 32, 31, 35, 50, 60], your program should be able to find that the subarray lies between the indexes 3 and 8.
2) If the input array is [0, 1, 15, 25, 6, 7, 30, 40, 50], your program should be able to find that the subarray lies between the indexes 2 and 5.


Sort an array in wave form
==========================
Given an unsorted array of integers, sort the array into a wave like array. An array ‘arr[0..n-1]’ is sorted in wave form if arr[0] >= arr[1] <= arr[2] >= arr[3] <= arr[4] >= …..

TO DO: linear time

K’th Smallest/Largest Element in Unsorted Array | (Worst Case Linear Time)
=========
TODO- linear time



Find the closest pair from two sorted arrays
=========
Given two sorted arrays and a number x, find the pair whose sum is closest to x and the pair has an element from each array.
Input:  ar1[] = {1, 4, 5, 7};
        ar2[] = {10, 20, 30, 40};
        x = 32      
Output:  1 and 30
TODO- linear time


Pattern Searching
=============================================================================
Naive algorithm
--------------
The number of comparisons in the worst case is O(m*(n-m+1)) where m is length of pattern and n is length of Text
Rabin-Karp Algorithm
--------------------
The average and best case running time of the Rabin-Karp algorithm is O(n+m), but its worst-case time is O(nm).
KMP algorithm
==============
https://www.youtube.com/watch?v=GTJr8OvyEVQ
Pattern preprocessing step 
time complexity O(m) and space complexity O(m)
example
a b c d a b c y
0 0 0 0 1 2 3 0
a a b a a b a a a
0 1 0 1 2 3 4 5 2
match time complexity = O(n+m)

TODO : write own code

Pattern Matching with Finite Automata
=====================================
https://www.youtube.com/watch?v=M_XpGQyyqIQ
Example : pattern ababc
transition table
    a  b  c
s0  s1 s0 s0
s1  s1 s2 s0
s2  s3 s0 s0
s3  s1 s4 s0
s4  s3 s0 s5
s5
tricky : longest suffix which is equal to longest prefix of pattern

time complexity of the computeTF() is O(m^3*NO_OF_CHARS) where m is length of the pattern and NO_OF_CHARS is size of alphabet (total number of possible characters in pattern and text)
The implementation tries all possible prefixes starting from the longest possible that can be a suffix of “pat[0..k-1]x”.

Time complixity searching pattern is O(n)

Improved this algorithm for construct Transition table O(M*NO_OF_CHARS)
=================================
int M = pat.length; 
int N = txt.length; 
int[][] TF = new int[M + 1][NO_OF_CHARS]; 

 static void computeTransFun(char[] pat,int M, int TF[][])  
    { 
        int i, lps = 0, x; 
        // Fill entries in first row  
        for (x = 0; x < NO_OF_CHARS; x++)  
        { 
            TF[0][x] = 0; 
        } 
        TF[0][pat[0]] = 1; 
  
        // Fill entries in other rows  
        for (i = 1; i < M; i++)  
        { 
            // Copy values from row at index lps  
            for (x = 0; x < NO_OF_CHARS; x++)  
            { 
                TF[i][x] = TF[lps][x]; 
            } 
            // Update the entry corresponding to this character  
            TF[i][pat[i]] = i + 1; 
            // Update lps for next row to be filled  
            if (i < M)  
            { 
                lps = TF[lps][pat[i]]; 
            } 
        } 
    } 


Boyer Moore Algorithm for Pattern Searching
=========================================
1) Bad Character Heuristic
---------------------------
class MyTest {

	static int NO_OF_CHARS = 256;

	// A utility function to get maximum of two integers
	static int max(int a, int b) {
		return (a > b) ? a : b;
	}

	// The preprocessing function for Boyer Moore's
	// bad character heuristic
	static void badCharHeuristic(char[] str, int size, int badchar[]) {
		int i;

		// Initialize all occurrences as -1
		for (i = 0; i < NO_OF_CHARS; i++)
			badchar[i] = -1;
		// Fill the actual value of last occurrence of a character
		for (i = 0; i < size; i++)
			badchar[(int) str[i]] = i;
	}

	static void search(char txt[], char pat[]) {
		int m = pat.length;
		int n = txt.length;

		int badchar[] = new int[NO_OF_CHARS];

		/*
		 * Fill the bad character array by calling the preprocessing function
		 * badCharHeuristic() for given pattern
		 */
		badCharHeuristic(pat, m, badchar);

		int s = 0; // s is shift of the pattern with
					// respect to text
		while (s <= (n - m)) {
			int j = m - 1;

			/*
			 * Keep reducing index j of pattern while characters of pattern and text are
			 * matching at this shift s
			 */
			while (j >= 0 && pat[j] == txt[s + j])
				j--;

			/*
			 * If the pattern is present at current shift, then index j will become -1 after
			 * the above loop
			 */
			if (j < 0) {
				System.out.println("Patterns occur at shift = " + s);

				/*
				 * Shift the pattern so that the next character in text aligns with the last
				 * occurrence of it in pattern. The condition s+m < n is necessary for the case
				 * when pattern occurs at the end of text
				 */
				s += (s + m < n) ? m - badchar[txt[s + m]] : 1;

			}

			else
				/*
				 * Shift the pattern so that the bad character in text aligns with the last
				 * occurrence of it in pattern. The max function is used to make sure that we
				 * get a positive shift. We may get a negative shift if the last occurrence of
				 * bad character in pattern is on the right side of the current character.
				 */
				s += max(1, j - badchar[txt[s + j]]);
		}
	}

	/* Driver program to test above function */
	public static void main(String[] args) {
		char txt[] = "ABAAABCD".toCharArray();
		char pat[] = "ABC".toCharArray();
		search(txt, pat);
	}
}

The Bad Character Heuristic may take O(mn) time in worst case. The worst case occurs when all characters of the text and pattern are same.

Good Suffix heuristic
----------------------
Let t be substring of text T which is matched with substring of pattern P. Now we shift pattern until :
1) Another occurrence of t in P matched with t in T.
2) A prefix of P, which matches with suffix of t
3) P moves past t


class MyTest 
{ 

// preprocessing for strong good suffix rule 
static void preprocess_strong_suffix(int []shift, int []bpos, 
									char []pat, int m) 
{ 
	// m is the length of pattern 
	int i = m, j = m + 1; 
	bpos[i] = j; 

	while(i > 0) 
	{ 
		/*if character at position i-1 is not 
		equivalent to character at j-1, then 
		continue searching to right of the 
		pattern for border */
		while(j <= m && pat[i - 1] != pat[j - 1]) 
		{ 
			/* the character preceding the occurrence of t 
			in pattern P is different than the mismatching 
			character in P, we stop skipping the occurrences 
			and shift the pattern from i to j */
			if (shift[j] == 0) 
				shift[j] = j - i; 

			//Update the position of next border 
			j = bpos[j]; 
		} 
		/* p[i-1] matched with p[j-1], border is found. 
		store the beginning position of border */
		i--; j--; 
		bpos[i] = j; 
	} 
} 

//Preprocessing for case 2 
static void preprocess_case2(int []shift, int []bpos, 
							char []pat, int m) 
{ 
	int i, j; 
	j = bpos[0]; 
	for(i = 0; i <= m; i++) 
	{ 
		/* set the border position of the first character 
		of the pattern to all indices in array shift 
		having shift[i] = 0 */
		if(shift[i] == 0) 
			shift[i] = j; 

		/* suffix becomes shorter than bpos[0], 
		use the position of next widest border 
		as value of j */
		if (i == j) 
			j = bpos[j]; 
	} 
} 

/*Search for a pattern in given text using 
Boyer Moore algorithm with Good suffix rule */
static void search(char []text, char []pat) 
{ 
	// s is shift of the pattern 
	// with respect to text 
	int s = 0, j; 
	int m = pat.length; 
	int n = text.length; 

	int []bpos = new int[m + 1]; 
	int []shift = new int[m + 1]; 

	//initialize all occurrence of shift to 0 
	for(int i = 0; i < m + 1; i++) 
		shift[i] = 0; 

	//do preprocessing 
	preprocess_strong_suffix(shift, bpos, pat, m); 
	preprocess_case2(shift, bpos, pat, m); 

	while(s <= n - m) 
	{ 
		j = m - 1; 

		/* Keep reducing index j of pattern while 
		characters of pattern and text are matching 
		at this shift s*/
		while(j >= 0 && pat[j] == text[s+j]) 
			j--; 

		/* If the pattern is present at the current shift, 
		then index j will become -1 after the above loop */
		if (j < 0) 
		{ 
			System.out.printf("pattern occurs at shift = %d\n", s); 
			s += shift[0]; 
		} 
		else
		
			/*pat[i] != pat[s+j] so shift the pattern 
			shift[j+1] times */
			s += shift[j + 1]; 
	} 

} 

public static void main(String[] args) 
{ 
	char []text = "ABAAAABAACD".toCharArray(); 
	char []pat = "ABA".toCharArray(); 
	search(text, pat); 
} 
} 



Anagram Substring Search (Or Search for all permutations)
==========================================================
Given a text txt[0..n-1] and a pattern pat[0..m-1], write a function search(char pat[], char txt[]) that prints all occurrences of pat[] and its permutations (or anagrams) in txt[]. You may assume that n > m.
Expected time complexity is O(n)

The important thing to note is, time complexity to compare two count arrays is O(1) as the number of elements in them are fixed (independent of pattern and text sizes). Following are steps of this algorithm.
1) Store counts of frequencies of pattern in first count array countP[]. Also store counts of frequencies of characters in first window of text in array countTW[].

2) Now run a loop from i = M to N-1. Do following in loop.
…..a) If the two count arrays are identical, we found an occurrence.
…..b) Increment count of current character of text in countTW[]
…..c) Decrement count of first character in previous window in countWT[]

3) The last window is not checked by above loop, so explicitly check it.

Aho-Corasick Algorithm for Pattern Searching
=============================================
https://www.youtube.com/watch?v=ePafMI_rSJg

https://www.youtube.com/watch?v=qPyhPXPl3T4

https://www.geeksforgeeks.org/aho-corasick-algorithm-pattern-searching/

Given an input text and an array of k words, arr[], find all occurrences of all words in the input text. Let n be the length of text and m be the total number characters in all words, i.e. m = length(arr[0]) + length(arr[1]) + … + length(arr[k-1]). Here k is total numbers of input words.

Example:

Input: text = "ahishers"    
       arr[] = {"he", "she", "hers", "his"}

Output:
   Word his appears from 1 to 3
   Word he appears from 4 to 5
   Word she appears from 3 to 5
   Word hers appears from 4 to 7
If we use a linear time searching algorithm like KMP, then we need to one by one search all words in text[]. This gives us total time complexity as O(n + length(word[0]) + O(n + length(word[1]) + O(n + length(word[2]) + … O(n + length(word[k-1]). This time complexity can be written as O(n*k + m).
Aho-Corasick Algorithm finds all words in O(n + m + z) time where z is total number of occurrences of words in text. The Aho–Corasick string matching algorithm formed the basis of the original Unix command fgrep.


kasai’s Algorithm for Construction of LCP array from Suffix Array
==================================================================
first learn what is kasai algorithm 
suffix array
TODO

Z algorithm (Linear time pattern searching Algorithm)
=====================================================
https://www.youtube.com/watch?v=CpZh4eF8QBw



